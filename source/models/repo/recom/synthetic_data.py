# -*- coding: utf-8 -*-
"""Synthetic_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DqMVPOTYIQx7enGGaFWBeS1hMWa6TS0K

# SDV
https://github.com/sdv-dev/SDV
# CTGAN
https://github.com/sdv-dev/CTGAN

## install SDV
"""

!pip install sdv

"""for this issue
**ValueError: max() arg is an empty sequence**
> we can solve it by the next installition cell

"""

!pip install ctgan==0.3.1.dev0





"""## link conda with colab
**Note:** Still didn't use it
"""

"""

https://serverfault.com/questions/43383/caching-preloading-files-on-linux-into-ram

"""


##### Access to conda environnment package  ################################################
import os, sys
! ls "/content/drive/MyDrive/Colab Notebooks/shared_session/bin/conda"
# from util_conda import conda_link_gdrive, conda_create_env, conda_install_pkg

def conda_link_gdrive():
  prefix = "/usr/local/"
  ### Linking to Conda on Google Drive
  ### conda_commands = r"/content/drive/My\ Drive/Colab\ Notebooks/shared_session/recsys/bin/conda/bin"
  conda_folder = r"/content/drive/My\ Drive/Colab\ Notebooks/shared_session//bin/conda"

  # Sym link conda with colab
  !ln -s  $conda_folder "/usr/local" #create symlink
  !ls /usr/local/conda/  &&  ls /usr/local/conda/envs/

  # give colab permission to use conda 
  !sudo chmod -R 777 /usr/local/conda/bin

  # add conda commands to system path
  os.environ['PATH'] += ":/usr/local/conda/bin"


def conda_create_env(env_name = "py36", python_version='3.6.9'):
    conda_envs = !conda env list
    res = [i for i in conda_envs if env_name in i]
    if (len(res) == 0):
        print('not found ' + env_name + ' env', len(res))
        !conda create -y -q --name $env_name python=$python_version 
    else:
        print('ALready found ' + env_name + ' env', len(res))

def conda_install_pkg(env_name = "py36",  install='', give_access=True):
    conda_envs = !conda env list
    res = [i for i in conda_envs if env_name in i]
    if (len(res) == 0):
        print('not found ' + env_name + ' env', len(res))
    else:
        print('found ' + env_name + ' env', len(res))
        if (give_access):
            !sudo chmod -R 755 /usr/local/conda/envs/$env_name/bin/

        if (len(install) > 0):
            if (install.find('.txt') > -1):
                # install files like requirments.txt
                !source activate $env_name  && pip install -r $install
            else:
                !source activate $env_name && pip install $install

def conda_change_env(env_name = "py36"):
    prefix = "/usr/local/"
    conda_envs = !conda env list
    res = [i for i in conda_envs if env_name in i]
    if (len(res) == 0):
        print('not found ' + env_name + ' env', len(res))
    else:
        print('found ' + env_name + ' env', len(env_name))
        # romoves all enviroment packeges to add new env
        for path in sys.path:
            if 'envs' in path:
                sys.path.remove(path)
        packages_path = f'{prefix}/conda/envs/{env_name}/lib/python3.6/site-packages'
        if packages_path not in sys.path:
            print("packages path not found")
            print("updating packages path....")
            sys.path.append( f'{prefix}/conda/envs/{env_name}/lib/python3.6/site-packages')
            # print(sys.path)
            sys.__plen = len(sys.path) - 1
            new=sys.path[sys.__plen:] 
            del sys.path[sys.__plen:] 
            p=getattr(sys,'__egginsert',0)
            p=0
            sys.path[p:p]=new
            sys.__egginsert = p+len(new)
            print(sys.path)
            print("packages path updated.")
            # ! pip install arrow
        else:
            print("packages path existed")

#### List conda conda envs from the Google Drive conda
conda_link_gdrive()

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# conda env list
#

##### Install package  ####################################
# env_name = 'py36rec'
# conda_install_pkg(env_name, 'tensorflow==1.12.3')


# %%bash
# source activate py36rec
# conda list
# conda install tensorflow==1.12.3 -n py36rec 
# pip install

# Commented out IPython magic to ensure Python compatibility.
# ##### Check package  ####################################
# %%bash
# source activate py36rec
# conda list
# #conda install tensorflow==1.12.3 -n py36rec 
# # pip install 
# 
#

env_name = 'py36rec'
conda_change_env(env_name)
print(sys.path)

##### Caching into RAM the folder

! find /usr/local/conda/envs/py36rec/lib/python3.6/site-packages/ -type f -exec cat {} \;  &>/dev/null









# Commented out IPython magic to ensure Python compatibility.
# """
# 
# find /usr/local/conda/envs/py36rec/lib/python3.6/site-packages/ -type f -exec cat {} \;  &>/dev/null
# 
# 
# command &>/dev/null
# 
# 
# %%bash
# 
# mkdir /usr/local/conda
# sudo mount -t tmpfs -o size=4G tmpfs '/usr/local/conda'
# 
# df
# 
# 
# !rsync -avh "/content/drive/MyDrive/Colab Notebooks/shared_session/bin/conda" /usr/local/
# 
# ! ls  '/content/drive/My Drive/colabs/'
# 
# 
# ! mkdir '/content/drive/My Drive/colabs/test2'
# 
# 
# 
# ! ls  '/content/drive/MyDrive/Colab Notebooks/shared_session/bin/conda/'
# 
# ! ls /dev/shm/
# 
# 
# %%bash
# 
# sudo mount -t tmpfs -o nonempty -o size=4G tmpfs "/content/drive/My Drive/Colab\ Notebooks/shared_session/bin/conda/"
# 
# df
# 
# """
# 
#





import importlib

import tensorflow as tf
# importlib.reload(tf)
print(tf.__version__)





"""# Single Table Data

"""

from sdv.demo import  _load_demo_dataset

"""https://sdv.dev/SDV/user_guides/index.html


https://hub.gke2.mybinder.org/user/sdv-dev-sdv-6gwj44tn/tree/tutorials

MTSS-GAN - Multivariate conditional time series simulation using stacked generative adversarial learning for multi-attribute generation.

Developed by Derek Snow @firmai

Time-GAN - Multivariate time series generation with an emphasis on autoregression errors to preserve temporal correlations.

Developed by Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar, forked code and colab hosted by @firmai

DoppleGANger - Modelling time series and mixed-type data

Cross-Sectional
Privacy
VAE-DP - Variational Autoencoder with Differential Privacy

Original model developed by MIT, differential privacy included by Derek Snow @firmai

PrivBN - Privbayes: Private data release via bayesian networks

General
CTGAN - Conditional GAN for Tabular Data

CTGAN:
Tabular data usually contains a mix of discrete and continuous columns. Continuous columns may have multiple modes whereas discrete columns are sometimes imbalanced making the modeling difficult. Existing statistical and deep neural network models fail to properly model this type of data. We design TGAN, which uses a conditional generative adversarial network to address these challenges. In CTGAN, we invent the mode-specific normalization (In CTGAN, we use variational Gaussian mixture model (VGM) to normalize continuous columns 25% improvement) to overcome the non-Gaussian and multimodal distribution (Section 4.2). We design a conditional generator (imbalanced data; 20% improvent) and training-by-sampling (conditional loss) to deal with the imbalanced discrete columns (Section 4.3). Ablation study necessary. They use WGANGP. The identity (original data) always had the best performance.

TVAE (Same Paper as CTGAN) - Variational autoencoder (VAE) for mixed-type tabular data generation. VAEs directly use data to build the generator;

TVAE:
Benefit here is that no conditioning is used for imbalanced data and this model still performs at a similar level to CTGAN above. On real datasets,TVAE and CTGAN outperform CLBN and PrivBN, whereas other GAN models cannot get as good a result as Bayesian networks.

TVAE outperforms CTGAN in several cases, but GANs do have several favorable attributes, and this does not indicate that we should always use VAEs rather than GANs to model tables. The generator in GANs does not have access to real data during the entire training process; thus, we can make CTGAN achieve differential privacy [14] easier than TVAE. (This might not be true, it has to be tested)

CLBN - Approximating discrete probability distributions with dependence trees

CLBN and PrivBN
For simulated data from Gaussian mixture,CLBNandPrivBNsuffer because continuousnumeric data has to be discretized before modeling using Bayesian networks. With respect to large scale real datasets, learning a high-quality Bayesian network is difficult. So models trained on CLBN and PrivBN synthetic data are 36.1% and 51.8% worse than models trained on real data.

TableGAN - Data synthesis based on generative adversarial networks

VEEGAN - Veegan: Reducing mode collapse in gans using implicit variational learning. Chris Russell - Alan Turing.

CTGAN:
A GAN variant that avoids mode collapse. Create a reconstruction network to map the data distribution to a Gaussian, but also to approximately reverse the action of the generator. Intuitively, if the reconstructor learns both to map all of the true data to the noise distribution and is an approximate inverse of the generator network, this will encourage the generator network to map from the noise distribution to the entirety of the true data distribution, thus resolving mode collapse. There was a previous iteration that did not use conditional GANS.

Other
Evaluation - A range of implementation and evaluation metrics as part of a masters thesis by Bauke Brenninkmeijer.

We always want to compare our method with benchmarks, first we want to use the original data as is used in training synthesisers, then we want to sample each column using a uniform distribution, second we can sample each column using a gaussian mixture model if continuous and a probability mass function if discrete.


"

### load **student_placements** Dataset
"""

from sdv.demo import load_tabular_demo

data = load_tabular_demo('student_placements')

## tell model this field is unique
primary_key = 'student_id'

data.head()

data._metadata







"""## load **Online Shopper Intention Dataset**"""

# Download online_shoppers_intention data
from urllib import request

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv"
path = "/content/online_shoppers_intention.csv"

request.urlretrieve (url, path)

# Read online_shoppers_intention data 
import pandas as pd

path = "/content/online_shoppers_intention.csv"
df = pd.read_csv(path)

primary_key = None

df.head()

cols = list( df.columns)

"""### Metadata"""

# metadata from online_shoppers_intention dataset 
# define path of data, tables 
# define each table name in tables key
# the table name values contain path, fields
# fields ==> {columns name ==> type, subtype}
MD = {
    'path':"online_shoppers_intention.csv",

    'tables':{
        'online_shoppers_intention': {
            'path':"online_shoppers_intention.csv",
            'fields': {
                'Administrative': {
                    'type': 'numerical',
                    'subtype':'integer'
                },
                'Administrative_Duration': {
                    'type': 'numerical',
                    'subtype':'float'

                },
                'Informational': {
                    'type': 'numerical',
                    'subtype':'integer'
                },
                'Informational_Duration': {
                    'type': 'numerical',
                    'subtype':'float'

                },
                'ProductRelated': {
                    'type': 'numerical',
                    'subtype':'integer'

                },
                'ProductRelated_Duration': {
                    'type': 'numerical',
                    'subtype':'float'

                },
                'BounceRates': {
                    'type': 'numerical',
                    'subtype':'float'

                },
                'ExitRates': {
                    'type': 'numerical',
                    'subtype':'float'

                },
                'PageValues': {
                    'type': 'numerical',
                    'subtype':'float'

                },
                'SpecialDay': {
                    'type': 'numerical',
                    'subtype':'float'

                },
                'Month': {
                    'type': 'categorical'
                },
                'OperatingSystems': {
                   'type': 'numerical',
                    'subtype':'integer'                  
                },
                'Browser': {
                    'type': 'numerical',
                    'subtype':'integer'
                    
                },
                'Region': {
                   'type': 'numerical',
                    'subtype':'integer'
                },
                'TrafficType': {
                    'type': 'numerical',
                    'subtype':'integer'
                },
                'VisitorType': {
                    'type': 'categorical'                    
                },
                'Weekend': {
                    'type': 'boolean'
                    },
                'Revenue': {
                    'type': 'boolean'
                    }
            }
        }
    }
    
}

# save dictionery as metadata.json file
import json
with open('metadata.json', 'w', encoding='utf-8') as f:
    json.dump(MD, f, ensure_ascii=False, indent=4)

"""### load data as sdv moudle did"""

# load data as sdv repo did 
from sdv.demo import _get_dataset_path, _dtypes64
from sdv.metadata import Metadata, Table
import os

dataset_name = 'online_shoppers_intention.csv'
data_path="/content/"

# get path of data
dataset_path = _get_dataset_path(dataset_name, data_path)

# Metadata class read json file that we created above which use this file to read all tables
meta = Metadata(metadata=os.path.join(data_path, 'metadata.json'))

# read each table from the Metadata class
tables = {
    name: _dtypes64(table)
    for name, table in meta.load_tables().items()
}

table_name = 'online_shoppers_intention'

# read specific table
table = _dtypes64(tables[table_name])

# meta ==> object form Metadata class that used to load data form metadata.json
# tables ==> dictionery loaded all tables in our sitution we have only one table 
# table ==> it is a dataframe we will use in training
meta, tables, table

# show metadata 
meta.to_dict()

# show each table and its fields 
{table: meta for table, meta in MD['tables'].items()
                if meta.pop('use', True)}

# print unique values for each columns
cols = list(table.columns)
uni_values = {}
for col_name in cols:
    uni_values[col_name] = table[col_name].unique()

uni_values

# constains 
# want to generate only positve values


# from sdv.constraints import UniqueCombinations, CustomConstraint, GreaterThan

# def pos_float(data):
#     return data['']

# unique_constraint = UniqueCombinations(
#      columns=['Month'],
#      handling_strategy='transform'
#  )

"""### Train shopper dataset"""



"""
We need to encode
   string category --> int values
   


"""

table.describe()

from sdv.tabular import CTGAN
# table_metadata=meta.to_dict()['tables'][table_name],

model = CTGAN(
               epochs=10,
              batch_size=100,
              generator_dim=(256, 256, 256),
              discriminator_dim=(256, 256, 256),
              verbose= True)

model1 = CTGAN(
               epochs=30,
              batch_size=100,
              generator_dim=(256, 256, 256),
              discriminator_dim=(256, 256, 256),
              verbose= True)

model2 = CTGAN(
               epochs=30,
              batch_size=100,
              generator_dim=(256, 256, 256),
              discriminator_dim=(256, 256, 256),
              verbose= True)

model.fit(table)

cols = list(table.columns)

cols1 =    cols[:int(len(cols)/2)]     ##  half of columns only
cols2 =    cols[int(len(cols)/2):]     ##  half of columns only


# # remove all floated columns 
# # cols1 = []
# # removed_indx = [1,3,5,6,7,8,9]
# # i = 0
# # for name in cols:
# #     if i not in removed_indx:
# #         cols1.append(name)
# #     i+=1


test1 = table[ cols1 ]# [ cols1 ].iloc[:1000] 
test2 = table[ cols2 ]

# # test.head()

# model1.fit(test1)
# model2.fit(test2)

model1.sample(10)

model2.sample(10)

table.head(10)

model.sample(5)

model.sample(3)

model.sample(3)

model.sample(3)

"""### Evaluate"""

! pip install sdmetrics

# evaluate each columns in real_data&synthetic_data
from sdmetrics.single_column import KSTest

data = table

cols = list(data.columns)
cols_score = {}

real_data = data
synthetic_data = model.sample(len(data))

for column_name in cols:
    print("> ", column_name)
    real_column = real_data[column_name].to_numpy()
    synthetic_column = synthetic_data[column_name].to_numpy()
    score = KSTest.compute(real_column, synthetic_column)
    cols_score[column_name] =  score
    print("res: ",score)

dic_sorted_cols = sorted(cols_score.items(),key=lambda x: x[1], reverse=True)
lis_sorted_cols  = [k for k,v in dic_sorted_cols]
lis_sorted_cols

dic_sorted_cols

test = table[ lis_sorted_cols [:5] ]

test.head()

model1.fit(test)

model1.sample(5)



import sdmetrics

real_data = table

synthetic_data = model.sample(len(real_data)) # Choose the model that you trained

metrics = sdmetrics.single_table.SingleTableMetric.get_subclasses() 
sdmetrics.compute_metrics(metrics, real_data, synthetic_data)

from sdv.evaluation import evaluate

evaluate(real_data, synthetic_data)





####  Evaluate
#### https://github.com/sdv-dev/SDMetrics

import sdmetrics
from sdmetrics.single_table import LogisticDetection

from sdmetrics import load_demo

real_data, synthetic_data, metadata = load_demo()

# real_table = real_data['users']

# synthetic_table = synthetic_data['users']

# LogisticDetection.compute(real_table, synthetic_table)


# Obtain the list of multi table metrics, which is returned as a dict
# containing the metric names and the corresponding metric classes.
metrics = sdmetrics.multi_table.MultiTableMetric.get_subclasses()

# Run all the compatible metrics and get a report
sdmetrics.compute_metrics(metrics, real_data, synthetic_data, metadata=metadata)

metadata

real_data







"""### time series"""

table.head()

table.columns

for col in table.columns:
    print(col,' : ',table[table.Administrative == 1][col].unique())

entity_columns = ['Administrative']

context_columns = []

sequence_index = None

from sdv.timeseries import PAR

model = PAR(
    entity_columns=entity_columns,
    context_columns=context_columns,
    sequence_index=sequence_index,
)


model.fit(table)

model.sample(5)

"""### time series evalution"""



num_seq = len(table.Administrative.unique()) 
tab_len = len(table)
seq_len = tab_len  / num_seq

tab_len, num_seq, seq_len

# id = 0
# real_data = table[table.Administrative == id]
# synthetic_data = model.sample(num_sequences=id + 1, sequence_length=int(len(real_data)))
# synthetic_data = synthetic_data[synthetic_data.Administrative == id]
# len(real_data), len(synthetic_data)

real_data = table.sort_values('Administrative', ascending=False,ignore_index=True)

    
synthetic_data = real_data.copy().iloc[0:0]
for i in real_data.Administrative.unique():
    ids = pd.DataFrame({
        'Administrative': [i]
    })
    l = len(real_data[real_data.Administrative == i])
    data_for_seq = model.sample(num_sequences=1, sequence_length= l ,context=ids)
    synthetic_data = pd.concat([synthetic_data, data_for_seq], ignore_index=True)
synthetic_data

len(synthetic_data)

real_data.groupby('Administrative').size()

synthetic_data.groupby('Administrative').size()

real_data = real_data.sort_values('Administrative', ascending=False,ignore_index=True)
synthetic_data = synthetic_data.sort_values('Administrative', ascending=False,ignore_index=True)

from sdv.metrics.timeseries import LSTMDetection, TSFCDetection

# LSTMDetection.compute(real_data, synthetic_data,entity_columns=entity_columns)

TSFCDetection.compute(real_data, synthetic_data, entity_columns=entity_columns)

from sdv.metrics.timeseries import TSFClassifierEfficacy


cols = list(data.columns)
cols_score = {}
metadata = meta.to_dict()['tables']['online_shoppers_intention']

for column_name in cols[1:]:
    metadata['target'] = column_name
    print("> ", column_name)
    # real_column = real_data[column_name].to_numpy()
    # synthetic_column = synthetic_data[column_name].to_numpy()
    score = TSFClassifierEfficacy.compute(real_data, synthetic_data, entity_columns=entity_columns, target=column_name)
    cols_score[column_name] =  score
    print("res: ",score)

synthetic_data



"""## CTGAN

### model
"""

from sdv.tabular import CTGAN

model = CTGAN(primary_key= primary_key,
               epochs=50,
              batch_size=100,
              generator_dim=(256, 256, 256),
              discriminator_dim=(256, 256, 256),
              verbose= True)

model.fit(data)



model.save('ctgan_model.pkl')



"""### Generate synthetic data from the model"""

new_data = model.sample(200)

new_data.head()

# test if generated primary key is unique or not 
# new_data.student_id.value_counts().max()



from sdv.evaluation import evaluate

new_data_ = model.sample(len(data))

evaluate(new_data_, data)











"""## TVAE"""

from sdv.demo import load_tabular_demo

data = load_tabular_demo('student_placements')

## tell model this field is unique
primary_key = 'student_id'

data.head()

"""### model"""

from sdv.tabular import TVAE

model = TVAE(primary_key= primary_key,
               epochs=500,
              batch_size=100)

model.fit(data)



model.save('tvae_model.pkl')



"""### Generate synthetic data from the model"""

new_data = model.sample(200)

new_data.head()

# test if generated primary key is unique or not 
new_data.student_id.value_counts().max()



from sdv.evaluation import evaluate

new_data_ = model.sample(len(data))

evaluate(new_data_, data)



"""### **TVAE MODEL WITH SHOPPER DATA**"""

import warnings 
warnings.filterwarnings('ignore')

from sdv.tabular import TVAE

model_shopper = TVAE(primary_key= primary_key_shopper,
               epochs=500,
              batch_size=100)

model_shopper.fit(df) #25 min

model_shopper.save('tvae_shopper_model.pkl')

# test if generated primary key is unique or not 
#new_data_shopper.id.value_counts().max()

new_data_shopper.head()

from sdv.evaluation import evaluate

new_data_shopper_ = model_shopper.sample(len(df))

evaluate(new_data_shopper_, df) # 6 min

metrics_names=['BNLogLikelihood', 'LogisticDetection', 'SVCDetection', 'GMLogLikelihood', 'CSTest', 'KSTest', 'KSTestExtended', 'ContinuousKLDivergence', 'DiscreteKLDivergence']
metrics = sdmetrics.single_table.SingleTableMetric.get_subclasses()

new_metrics={k:v for k,v in metrics.items() if k in metrics_names}

sdmetrics.compute_metrics(new_metrics, df, new_data_shopper_) # remove-nan

new_data_shopper = model_shopper.sample(2000)

sdmetrics.compute_metrics(new_metrics, df, new_data_shopper) #small-new data

small_data=df.sample(2000)
small_model = TVAE(primary_key= primary_key_shopper,
               epochs=500,
              batch_size=100)

small_model.fit(small_data)

small_new_data = small_model.sample(2000)

sdmetrics.compute_metrics(new_metrics, small_data, small_new_data)







"""## GaussianCopula

### model
"""

from sdv.tabular import GaussianCopula

model = GaussianCopula(primary_key= primary_key)

model.fit(data)



model.save('gaussiancopula_model.pkl')



"""### Generate synthetic data from the model"""

new_data = model.sample(200)

new_data.head()

# test if generated primary key is unique or not 
new_data.student_id.value_counts().max()



from sdv.evaluation import evaluate

new_data_ = model.sample(len(data))

evaluate(new_data_, data)



"""## CopulaGAN

### model
"""

from sdv.tabular import CopulaGAN

model = CopulaGAN(primary_key= primary_key,
               epochs=500,
              batch_size=100,
              generator_dim=(256, 256, 256),
              discriminator_dim=(256, 256, 256))

model.fit(data)



model.save('copulagAN_model.pkl')



"""### Generate synthetic data from the model"""

new_data = model.sample(200)

new_data.head()

# test if generated primary key is unique or not 
new_data.student_id.value_counts().max()



from sdv.evaluation import evaluate

new_data_ = model.sample(len(data))

evaluate(new_data_, data)



"""#  Benchmark

"""





































# Commented out IPython magic to ensure Python compatibility.
# confogration check repo readme file and toutrial link above. 
model='HRM'

#### Data
splitter = 'loo';    ### How the data is split
dataset_name = 'ml-100k'  # 'Dataset name in dataset folder
format='UIRT'   #Stands for User Item Rating Time

### Train
epochs=2
time = False
# --data.convert.separator=';'




# Training model
# %run main.py --recommender=$model --learning_rate=0.001 --batch_size=128   --data.input.dataset=$dataset_name   --slitter=$splitter  --data.column.format=$format --epochs=$epochs --by_time=$time



























# Commented out IPython magic to ensure Python compatibility.
# confogration check repo readme file and toutrial link above. 
model='MLP'

#### Data
splitter = 'loo';    ### How the data is split
dataset_name = 'ml-100k' # 'ml-100k' #'custmized_dataset' #custmized_time_dataset
format='UIRT'

### Train
epochs=2
time = False
# --data.convert.separator=';'




# Training model
# %run main.py --recommender=$model --learning_rate=0.001 --batch_size=128   --data.input.dataset=$dataset_name   --slitter=$splitter  --data.column.format=$format --epochs=$epochs --by_time=$time







# Commented out IPython magic to ensure Python compatibility.
# confogration check repo readme file and toutrial link above. 
model='FISM'

#### Data
splitter = 'loo';    ### How the data is split
dataset_name = 'ml-100k'  # 'Dataset name in dataset folder
format='UIRT'   #Stands for User Item Rating Time

### Train
epochs=2
time = False
# --data.convert.separator=';'




# Training model
# %run main.py --recommender=$model --learning_rate=0.001 --batch_size=128   --data.input.dataset=$dataset_name   --slitter=$splitter  --data.column.format=$format --epochs=$epochs --by_time=$time









# Commented out IPython magic to ensure Python compatibility.
# confogration check repo readme file and toutrial link above. 
model='NAIS'

#### Data
splitter = 'loo';    ### How the data is split
dataset_name = 'ml-100k'  # 'Dataset name in dataset folder
format='UIRT'   #Stands for User Item Rating Time

### Train
epochs=2
time = False
# --data.convert.separator=';'




# Training model
# %run main.py --recommender=$model --learning_rate=0.001 --batch_size=128   --data.input.dataset=$dataset_name   --slitter=$splitter  --data.column.format=$format --epochs=$epochs --by_time=$time













# Commented out IPython magic to ensure Python compatibility.
# confogration check repo readme file and toutrial link above. 
model='DeepICF'

#### Data
splitter = 'loo';    ### How the data is split
dataset_name = 'ml-100k'  # 'Dataset name in dataset folder
format='UIRT'   #Stands for User Item Rating Time

### Train
epochs=2
time = False
# --data.convert.separator=';'




# Training model
# %run main.py --recommender=$model --learning_rate=0.001 --batch_size=128   --data.input.dataset=$dataset_name   --slitter=$splitter  --data.column.format=$format --epochs=$epochs --by_time=$time











# Commented out IPython magic to ensure Python compatibility.
# confogration check repo readme file and toutrial link above. 
model='MF'

#### Data
splitter = 'loo';    ### How the data is split
dataset_name = 'ml-100k'  # 'Dataset name in dataset folder
format='UIRT'   #Stands for User Item Rating Time

### Train
epochs=2
time = False
# --data.convert.separator=';'




# Training model
# %run main.py --recommender=$model --learning_rate=0.001 --batch_size=128   --data.input.dataset=$dataset_name   --slitter=$splitter  --data.column.format=$format --epochs=$epochs --by_time=$time











# Commented out IPython magic to ensure Python compatibility.
# confogration check repo readme file and toutrial link above. 
model='LightGCN'

#### Data
splitter = 'loo';    ### How the data is split
dataset_name = 'ml-100k'  # 'Dataset name in dataset folder
format='UIRT'   #Stands for User Item Rating Time

### Train
epochs=2
time = False
# --data.convert.separator=';'




# Training model
# %run main.py --recommender=$model --learning_rate=0.001 --batch_size=128   --data.input.dataset=$dataset_name   --slitter=$splitter  --data.column.format=$format --epochs=$epochs --by_time=$time











# Commented out IPython magic to ensure Python compatibility.
# confogration check repo readme file and toutrial link above. 
model='SBPR'

#### Data
splitter = 'loo';    ### How the data is split
dataset_name = 'ml-100k' # 'ml-100k' #'custmized_dataset' #custmized_time_dataset
format='UIRT'

### Train
epochs=2
time = False
# --data.convert.separator=';'




# Training model
# %run main.py --recommender=$model --learning_rate=0.001 --batch_size=128   --data.input.dataset=$dataset_name   --slitter=$splitter  --data.column.format=$format --epochs=$epochs --by_time=$time





fields': [
                            {
                    'type': 'integer',
                    'name':'Administrative'
                },
                {
                    'type': 'numerical',
                    'name':'Administrative_Duration'
                },
                {
                    'type': 'integer',
                 'name':'Informational'
                },
                {
                    'type': 'numerical',
                    'name':'Informational_Duration'
                },
                 {
                    'type': 'integer',
                    'name':'ProductRelated'
                },
                 {
                    'type': 'numerical',
                    'name': 'ProductRelated_Duration'
                },{
                    'type': 'numerical',
                    'name': 'BounceRates'
                },
                 {
                    'type': 'numerical',
                    'name': 'ExitRates'
                },
                 {
                    'type': 'numerical',
                    'name':'PageValues'
                },
                {
                    'type': 'numerical',
                    'name':'SpecialDay'
                },
                {
                    'type': 'categorical',
                    'name':'Month'
                },
                {
                    'type': 'integer',
                    'name':'OperatingSystems'
                    
                },
                 {
                    'type': 'integer',
                  'name':'Browser'
                    
                },
                 {
                    'type': 'integer',
                    'name': 'Region'
                                    
                },
                 {
                    'type': 'integer',
                    'name':'TrafficType'
                },
                {
                    'type': 'categorical'  ,
                    'name':'VisitorType'              
                },
                {
                    'type': 'boolean',
                    'name': 'Weekend'
                    },
                 {
                    'type': 'boolean',
                    'name': 'Revenue'
                    }
            
            ]